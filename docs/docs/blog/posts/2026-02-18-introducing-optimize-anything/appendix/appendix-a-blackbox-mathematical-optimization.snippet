<span id="appendix-a-blackbox-mathematical-optimization"></span>
??? example "Blackbox Mathematical Optimization (EvalSet)"

    Here, we optimize a blackbox solver to minimize polynomial benchmark functions from the Evalset suite, benchmarked by Optuna.

    **Candidate** — The seed candidate is a minimal random-search solver: it samples a single point uniformly at random within the given bounds, evaluates it, and returns the result. This provides a starting point and a template for the API.

    ```python
    SEED_CODE = """
    import numpy as np
    
    def solve(objective_function, config, best_xs=None):
        bounds = np.array(config['bounds'])
        x = np.random.uniform(bounds[:, 0], bounds[:, 1])
        score = objective_function(x)
        all_attempts = [{"x": x.copy(), "score": score})]

        return {"x": x, "score": score, "all_attempts": all_attempts}
    """
    ```

    **Evaluator** — The evaluator sandboxes the proposed solver code, runs it on a benchmark function, and returns rich **Actionable Side Information (ASI)**: trial attempts, stdout/stderr, tracebacks, and budget status. A `BudgetTracker` helper distributes whatever evaluation budget is left across the remaining proposals, accounting for failures gracefully. `opt_state` passed through the `evaluate` function tracks the full optimization state; here we extract the best trials so far for warm-starting new proposals.

    ```python
    from gepa.examples.mathematical_optimization.utils import BudgetTracker

    num_proposals = 10
    budget = BudgetTracker(total_budgets=2000)

    def evaluate(candidate, opt_state):
        if budget.remaining <= 0:
            return -1e9, {"score": -1e9, "Error": "No budget remaining"}

        code = candidate["code"]
        best_xs = extract_best_xs(opt_state)  # warm-start from prior evaluations
        result = execute_code(code, problem_index=0, budget=100, best_xs=best_xs)
        budget.record(result)

        return result["score"], {
            "score": result["score"],
            "all_trials": result["all_trials"],
            "stdout": result.get("stdout", ""),
            "error": result.get("error", ""),
            "traceback": result.get("traceback", ""),
            "budget_total": budget.total,
            "budget_used": budget.used,
            "proposal_total": num_proposals,
            "proposal_completed": budget.candidates,
        }
    ```

    **Optimizer** -  This is a Single-Task Search mode: no `dataset` needed, just a natural-language `objective`. `background` lets you inject domain knowledge and constraints into the optimization loop. `cache_evaluation` caches scores and side information, saving metric calls on repeated evaluations. `max_candidate_proposals` caps the total number of proposals generated across the entire run.
    
    ```python
    from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig

    optimize_anything(
        seed_candidate={"code": SEED_CODE},
        evaluator=evaluate,
        config=GEPAConfig(
            engine=EngineConfig(max_candidate_proposals=10, cache_evaluation=True),
            reflection=ReflectionConfig(reflection_lm="openai/gpt-5"),
        ),
        objective="Evolve Python code that minimizes a blackbox objective function "
                  "using the available evaluation budget efficiently.",
        background="# Optional background, code requirements, strategies, etc..."
    )
    ```

    **Optimized artifact** — Here is the best artifact produced for P31: McCourt20. 250+ lines of solver code, fully generated by `optimize_anything`.

    <section class="scrollable-code" markdown>

    ```python
    import numpy as np
    from scipy.optimize import minimize
    from scipy.spatial.distance import cdist

    try:
        from sklearn.gaussian_process import GaussianProcessRegressor
        from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel
        SKLEARN_AVAILABLE = True
    except Exception:
        SKLEARN_AVAILABLE = False


    def solve(objective_function, config, best_xs=None):
        bounds = np.array(config['bounds'], dtype=float)
        dim = config.get('dim', bounds.shape[0])
        budget = int(config.get('budget', 100))
        all_attempts = []

        # Helper: clamp to bounds
        def clip_to_bounds(x):
            return np.clip(x, bounds[:, 0], bounds[:, 1])

        # Evaluation accounting
        eval_count = 0
        best_score = np.inf
        best_x = None

        def eval_obj(x):
            nonlocal eval_count, best_score, best_x
            if eval_count >= budget:
                return float(best_score if best_score < np.inf else 1e30)
            x = np.asarray(x, dtype=float)
            x_clipped = clip_to_bounds(x)
            score = float(objective_function(x_clipped))
            eval_count += 1
            all_attempts.append({"x": x_clipped.copy(), "score": score})
            if score < best_score:
                best_score = score
                best_x = x_clipped.copy()
            return score

        # Incorporate free previous data
        X_hist = []
        y_hist = []
        if best_xs:
            for item in best_xs:
                x_prev = np.asarray(item["x"], dtype=float)
                s_prev = float(item["score"])
                x_prev = clip_to_bounds(x_prev)
                all_attempts.append({"x": x_prev.copy(), "score": s_prev})
                X_hist.append(x_prev)
                y_hist.append(s_prev)
                if s_prev < best_score:
                    best_score = s_prev
                    best_x = x_prev.copy()
        X_hist = np.asarray(X_hist) if len(X_hist) > 0 else None
        y_hist = np.asarray(y_hist) if len(y_hist) > 0 else None

        # Handle degenerate budget
        if budget <= 0:
            if best_x is None:
                x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)
                return {"x": x0, "score": float("inf"), "all_attempts": all_attempts}
            return {"x": best_x, "score": best_score, "all_attempts": all_attempts}

        # Small budget: use cheap, simple strategy
        if budget <= 5:
            # If no evaluated point yet, force one evaluation at the center
            if best_x is None:
                x0 = np.mean(bounds, axis=1)
                eval_obj(x0)
            remaining = budget - eval_count
            for _ in range(max(0, remaining)):
                x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)
                eval_obj(x0)
            return {"x": best_x, "score": best_score, "all_attempts": all_attempts}

        # Main strategy: global surrogate-guided + local refinement
        remaining_budget = budget - eval_count

        # Use 40% for surrogate-guided global search, 60% for local search
        global_budget = max(5, int(0.4 * remaining_budget))
        global_budget = min(global_budget, remaining_budget - 3)  # leave some for local
        if global_budget < 0:
            global_budget = 0
        local_budget = budget - eval_count - global_budget

        # ---- Surrogate-guided global search ----
        # We will iteratively:
        # 1) sample a batch of candidate points
        # 2) use surrogate to score/explore them
        # 3) evaluate a few best candidates with true objective
        # This is robust, avoids expensive inner optimizations.
        def build_gp(X, y):
            if not SKLEARN_AVAILABLE or X is None or len(X) < 5:
                return None
            try:
                # Normalize outputs for stability
                y_mean = np.mean(y)
                y_std = np.std(y) if np.std(y) > 1e-12 else 1.0
                y_norm = (y - y_mean) / y_std

                kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(dim), nu=2.5) \
                        + WhiteKernel(noise_level=1e-6, noise_level_bounds=(1e-9, 1e-2))
                gp = GaussianProcessRegressor(
                    kernel=kernel,
                    alpha=0.0,
                    normalize_y=False,
                    n_restarts_optimizer=2,
                    random_state=None,
                )
                gp.fit(X, y_norm)
                gp.y_mean_ = y_mean
                gp.y_std_ = y_std
                return gp
            except Exception:
                return None

        def gp_predict(gp, Xc):
            # Return mean and std in original scale
            mu_norm, std_norm = gp.predict(Xc, return_std=True)
            mu = mu_norm * gp.y_std_ + gp.y_mean_
            std = std_norm * gp.y_std_
            return mu, std

        # Start with combined history: previous + any evaluations done so far (none yet here)
        X_sur = X_hist.copy() if X_hist is not None else None
        y_sur = y_hist.copy() if y_hist is not None else None

        while global_budget > 0 and eval_count < budget:
            # Build/update surrogate
            gp = build_gp(X_sur, y_sur) if X_sur is not None and len(X_sur) >= 5 else None

            # Number of real evaluations this iteration
            batch_eval = min(5, global_budget)

            # Generate candidate pool
            n_candidates = max(50, 10 * dim)
            rand_candidates = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_candidates, dim))

            # Add some perturbations of best point if available
            if best_x is not None:
                n_perturb = min(20, n_candidates // 4)
                scale = (bounds[:, 1] - bounds[:, 0]) * 0.05
                pert = best_x + np.random.randn(n_perturb, dim) * scale
                pert = clip_to_bounds(pert)
                cand = np.vstack([rand_candidates, pert])
            else:
                cand = rand_candidates

            # Remove points very close to already sampled (in X_sur)
            if X_sur is not None and len(X_sur) > 0:
                dists = cdist(cand, X_sur)
                min_d = dists.min(axis=1)
                mask = min_d > 1e-6 * np.sqrt(dim)
                cand = cand[mask]
                if len(cand) == 0:
                    cand = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_candidates, dim))

            # Score candidates using acquisition function
            if gp is not None and best_score < np.inf:
                mu, std = gp_predict(gp, cand)
                # Expected Improvement (EI) with jitter
                eps = 1e-9
                z = (best_score - mu - 1e-6) / (std + eps)
                from scipy.stats import norm
                ei = (best_score - mu) * norm.cdf(z) + std * norm.pdf(z)
                # Prefer high EI; break ties by lower mu
                order = np.argsort(-ei + 1e-9 * mu)
            else:
                # Fallback: purely random with slight exploitation toward best_x
                if best_x is not None:
                    # Score by distance to best_x for mild exploitation
                    d = np.linalg.norm(cand - best_x, axis=1)
                    order = np.argsort(d)
                    # Reverse to explore first (farthest), then close
                    order = order[::-1]
                else:
                    order = np.arange(len(cand))
                    np.random.shuffle(order)

            # Evaluate top batch_eval candidates
            selected = cand[order[:batch_eval]]
            for x0 in selected:
                if eval_count >= budget or global_budget <= 0:
                    break
                score = eval_obj(x0)
                global_budget -= 1
                # Update surrogate data
                if X_sur is None:
                    X_sur = np.asarray(x0, dtype=float).reshape(1, -1)
                    y_sur = np.array([score], dtype=float)
                else:
                    X_sur = np.vstack([X_sur, x0])
                    y_sur = np.append(y_sur, score)

        # ---- Local refinement using L-BFGS-B from diverse seeds ----
        if eval_count >= budget or local_budget <= 0:
            if best_x is None:
                x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)
                eval_obj(x0)
            return {"x": best_x, "score": best_score, "all_attempts": all_attempts}

        # Build starting points:
        start_points = []

        if best_x is not None:
            start_points.append(best_x.copy())

        # Add good points from history / surrogate data
        pool_X = []
        pool_y = []
        if X_sur is not None and len(X_sur) > 0:
            pool_X = X_sur
            pool_y = y_sur
        elif X_hist is not None and len(X_hist) > 0:
            pool_X = X_hist
            pool_y = y_hist

        if len(pool_X) > 0:
            idx_sorted = np.argsort(pool_y)
            used = []
            for idx in idx_sorted:
                x_cand = pool_X[idx]
                if len(used) == 0:
                    used.append(x_cand)
                    start_points.append(x_cand.copy())
                else:
                    d = np.linalg.norm(np.array(used) - x_cand, axis=1)
                    if np.min(d) > 1e-3 * np.sqrt(dim):
                        used.append(x_cand)
                        start_points.append(x_cand.copy())
                if len(start_points) >= 5:
                    break

        # Add random seeds if needed
        while len(start_points) < 3:
            x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)
            start_points.append(x0)

        # Distribute remaining budget across starts
        remaining_budget = budget - eval_count
        if remaining_budget <= 0:
            return {"x": best_x, "score": best_score, "all_attempts": all_attempts}

        n_starts = len(start_points)
        maxiter_per_start = max(1, remaining_budget // (2 * n_starts))
        # Keep at least one evaluation per start
        if maxiter_per_start < 3:
            maxiter_per_start = 3

        scipy_bounds = [(float(b[0]), float(b[1])) for b in bounds]

        for x0 in start_points:
            if eval_count >= budget:
                break

            def fun_wrapped(x):
                return eval_obj(x)

            try:
                minimize(
                    fun_wrapped,
                    x0=x0,
                    method="L-BFGS-B",
                    bounds=scipy_bounds,
                    options={"maxiter": maxiter_per_start, "disp": False},
                )
            except Exception:
                continue

        if best_x is None:
            x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)
            eval_obj(x0)

        return {"x": best_x, "score": best_score, "all_attempts": all_attempts}
    ```
    </section>

