<span id="appendix-f-arc-agi-agent-architecture-discovery"></span>
??? example "Agent Architecture Discovery (ARC-AGI)"

    Here, we tackle ARC-AGI1. This is a Generalization mode where the entire agent architecture is the artifact being optimized. The seed is a 10-line naive agent that makes a single LLM call; GEPA evolves it into a 170+ line multi-stage system with rule induction, code verification, iterative refinement, and structured fallbacks. Test accuracy improves from 32.5% to 89.5% on a public v1 test set.

    **Candidate** — The seed candidate is a minimal 10-line agent that concatenates training examples into a single prompt and asks the LLM to predict outputs directly. It provides a starting template showing the `solve()` API.

    ```python
    SEED_AGENT = '''
    import json, re
    def solve(train_inputs, train_outputs, test_inputs, llm):
        examples = "\\n".join(f"Input: {i}\\nOutput: {o}"
                              for i, o in zip(train_inputs, train_outputs))
        response = llm(f"Solve an ARC AGI puzzle. Training:\\n{examples}\\n"
                       f"Predict outputs as JSON [[...]]:")
        grids = [json.loads(g) for g in re.findall(r"\\[\\[.*?\\]\\]",
                 response.replace("\\n", ""))]
        return {"train": grids[:len(train_inputs)],
                "test": [[g] for g in grids[len(train_inputs):]]}
    '''
    ```

    **Evaluator** — The evaluator sandboxes the agent code, runs it on an ARC-AGI puzzle (providing training input/output pairs and test inputs), and returns rich ASI: training and test scores, execution errors, the actual grid examples, LLM costs, number of calls made, and all model outputs produced inside the agentic architecture. This lets the proposer see not just *what* the agent got wrong, but *how* it reasoned internally.

    ```python
    def evaluate(candidate, example):
        result = run_agent(
            agent_code=candidate,
            train_in=example.train_in,
            train_out=example.train_out,
            test_in=example.test_in,
            test_out=example.test_out or None,
            model_id=LLM_MODEL,
            max_llm_calls=MAX_LLM_CALLS,
        )
        llms = result["llms"]
        score = result["test_score"]

        return score, {
            "score": score,
            "problem_id": example.problem_id,
            "agent_code": candidate,
            "training_score": result["training_score"],
            "test_score": result["test_score"],
            "cost": llm.total_cost,
            "error": result["error"],
            "train_examples": result["train_examples"],
            "test_examples": result["test_examples"],
            **llms.get_traces(),  # number of calls made, LLM costs, model outputs, etc.
        }
    ```

    **Optimizer** — This is **Generalization** mode with `dataset` for training and `valset` for validation. The agent must generalize to unseen `testset` puzzles, so just memorizing patterns from the training set won't help. `parallel=True` with `max_workers=64` enables massive concurrent evaluation across puzzles. `background` provides domain knowledge about ARC puzzle structure. Gemini 3 Flash is used as both the reflection model and the agent's internal LLM. Note that using a stronger reflection model can find a even more effective artifact in general. 

    ```python
    from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig
    from examples.arc_agi.utils import load_arc_dataset

    train_set, val_set, test_set = load_arc_dataset()

    result = optimize_anything(
        seed_candidate={"agent_code": SEED_AGENT},
        evaluator=evaluate,
        dataset=train_set,
        valset=val_set,
        config=GEPAConfig(
            engine=EngineConfig(max_candidate_proposals=40, parallel=True,
                                max_workers=64, cache_evaluation=True),
            reflection=ReflectionConfig(
                reflection_lm="openrouter/google/gemini-3-flash-preview",
            ),
        ),
        objective="Evolve agent code to solve ARC-AGI puzzles. The agent receives "
                 "training input/output pairs and must predict test outputs.",
        background="You are allowed to build an agent system with up to 10 LLM calls and total of $0.8~1.0 LLM cost per problem.",
    )
    ```

    **Optimized artifact** — Starting from a 10-line single-call agent, GEPA discovered a 4-stage pipeline on its own: Analyst (infer the rule), Developer (write a `transform()` function), Validator (run it on all training examples and iteratively debug), and Optimizer (assemble attempts with fallbacks). What's interesting is that the agent learned to generate code, execute it, check the output, and fix bugs in a loop. It also learned a dual-attempt strategy: try the code path first, fall back to direct LLM prediction if that fails.

    <section class="scrollable-code" markdown>

    ````python
    import json
    import re
    import numpy as np

    def solve(train_inputs, train_outputs, test_inputs, llm):
        """
        ARC-AGI solver using a multi-stage reasoning and execution pipeline:
        1. Analyst: Infers transformation logic and describes it.
        2. Developer: Writes a Python function to implement the logic.
        3. Validator: Tests the code against ALL training examples and iterates if it fails.
        4. Optimizer: Uses the best-performing code or falls back to direct prediction via LLM.
        """

        def format_grid(grid):
            return json.dumps(grid)

        training_exs = ""
        for idx, (i, o) in enumerate(zip(train_inputs, train_outputs)):
            training_exs += f"Example {idx}:\nInput: {format_grid(i)}\nOutput: {format_grid(o)}\n\n"

        # Stage 1: Initial Programming Attempt
        programmer_prompt = f"""You are an absolute expert programmer and ARC-AGI solver.
    Analyze these training examples and identify the transformation rule.
    Consider: object properties (color, shape, position), grid symmetry, relative movement, and color mapping.

    {training_exs}

    Task:
    Write a Python function `transform(grid)` using numpy. 
    The function should return the transformed grid as a list of lists.
    Ensure your code is robust and handles grid boundaries.

    ```python
    import numpy as np

    def transform(grid):
        grid = np.array(grid)
        # logic here
        return grid.tolist()
    ```
    """

        response = llm(programmer_prompt)
        
        def extract_code(text):
            code_match = re.search(r"```python\s*(.*?)\s*```", text, re.DOTALL)
            return code_match.group(1) if code_match else ""

        code = extract_code(response)
        
        # Stage 2: Code Validation and Auto-Correction
        max_fix_attempts = 2
        best_code = code

        for _ in range(max_fix_attempts):
            success_count = 0
            execution_feedback = ""
            
            if not best_code:
                break

            try:
                # Create a namespace for the function
                namespace = {}
                exec("import numpy as np", namespace)
                exec(best_code, namespace)
                transform_fn = namespace.get('transform')
                
                if not transform_fn:
                    raise Exception("Function 'transform' not found in code.")

                for i, (in_grid, out_grid) in enumerate(zip(train_inputs, train_outputs)):
                    pred = transform_fn(in_grid)
                    if pred == out_grid:
                        success_count += 1
                    else:
                        execution_feedback += f"Example {i} failed. Expected {format_grid(out_grid)}, but got {format_grid(pred)}.\n"
            except Exception as e:
                execution_feedback = f"Error during execution: {str(e)}"

            if success_count == len(train_inputs):
                break
            else:
                # Code failed training; ask the LLM to fix it using feedback
                fixer_prompt = f"""The previous code failed validation.
    Rule Analysis: {response}

    Current Code:
    ```python
    {best_code}
    ```

    Validation Feedback:
    {execution_feedback}

    Correct the logic based on the feedback. Ensure it passes ALL training examples.
    {training_exs}
    Only provide the corrected ```python block.
    """
                response = llm(fixer_prompt)
                best_code = extract_code(response)

        # Stage 3: Execution and Fallback Generation
        final_test_results = []
        code_functional = False
        
        # Attempt to execute best_code on test inputs
        try:
            namespace = {}
            exec("import numpy as np", namespace)
            exec(best_code, namespace)
            transform_fn = namespace['transform']
            
            code_test_outputs = []
            for t_in in test_inputs:
                code_test_outputs.append(transform_fn(t_in))
            code_functional = True
        except:
            code_functional = False

        # Stage 4: Logical Reasoning Fallback (for the 2nd attempt or if code fails)
        fallback_prompt = f"""The pattern is based on these examples:
    {training_exs}

    Predict the output for these test inputs:
    {[format_grid(t) for t in test_inputs]}

    Respond ONLY with a JSON list of grids:
    ```json
    [[grid1], [grid2], ...]
    ```
    """
        fallback_response = llm(fallback_prompt)

        def extract_json_grids(text):
            json_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
            if json_match:
                try:
                    data = json.loads(json_match.group(1))
                    return data if isinstance(data, list) else []
                except: pass
            return []

        fallback_grids = extract_json_grids(fallback_response)

        # Assemble 2 attempts for each test input
        for idx, t_in in enumerate(test_inputs):
            attempts = []
            
            # 1st Attempt: Code output (if functional) or first fallback
            if code_functional and idx < len(code_test_outputs):
                attempts.append(code_test_outputs[idx])
            elif idx < len(fallback_grids):
                attempts.append(fallback_grids[idx])
            else:
                attempts.append(t_in) # Safety
                
            # 2nd Attempt: Top fallback or a modified version of the first
            if idx < len(fallback_grids) and fallback_grids[idx] not in attempts:
                attempts.append(fallback_grids[idx])
            
            # Padding to 2 attempts
            while len(attempts) < 2:
                attempts.append(attempts[0])
                
            final_test_results.append(attempts[:2])

        return {
            "train": [t for t in train_outputs],
            "test": final_test_results
        }
    ````
    </section>
